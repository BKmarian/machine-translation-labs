{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab2_MT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bucuram/machine-translation-labs/blob/main/Lab2_MT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjiNn4XDtESN"
      },
      "source": [
        "##Overview of Approaches to MT\n",
        "\n",
        "### Open NMT frameworks\n",
        "* [Moses](http://www.statmt.org/moses/https://aclanthology.org/P07-2045.pdf). Paper: [Moses: Open Source Toolkit for Statistical Machine Translation](https://aclanthology.org/P07-2045.pdf). C++\n",
        "\n",
        "* [OpenNMT](https://github.com/OpenNMT/OpenNMT-py). Paper: [OpenNMT: Open-Source Toolkit for Neural Machine Translation](https://aclanthology.org/P17-4012.pdf). PyTorch / TensorFlow. Developed by Harvard NLP,  SYSTRAN\n",
        "* [Marian](https://marian-nmt.github.io/). Paper: [Marian: Fast Neural Machine Translation in C++](https://aclanthology.org/P18-4020.pdf). C++. Developed by Microsoft Translator\n",
        "* [Fairseq](https://github.com/pytorch/fairseq). Paper: [FAIRSEQ: A Fast, Extensible Toolkit for Sequence Modeling](https://aclanthology.org/N19-4009.pdf). PyTorch. Developed by Facebook AI\n",
        "* [Nematus](https://github.com/EdinburghNLP/nematus). Paper: [Nematus: a Toolkit for Neural Machine Translation](https://aclanthology.org/E17-3017.pdf). TensorFlow. Developed by Edinburgh NLP\n",
        "* [Sockeye](https://github.com/awslabs/sockeye). Paper: [SOCKEYE 2:A Toolkit for Neural Machine Translation](https://aclanthology.org/2020.eamt-1.50.pdf). MXNet. Developed by Amazon\n",
        "* [JoeyNMT](https://github.com/joeynmt/joeynmt). Paper: [Joey NMT: A Minimalist NMT Toolkit for Novices](https://aclanthology.org/D19-3019v1.pdf). PyTorch\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efi_dqwm-8fb"
      },
      "source": [
        "###Exploring the fairseq framework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSTgKz3cBlXa"
      },
      "source": [
        "Installing `fairseq`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jw6BQ2aELpF4"
      },
      "source": [
        "!pip install fairseq "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKzw-dpE_Kaz"
      },
      "source": [
        "###Downloading the data\n",
        "\n",
        "We will use the Europarl parallel corpus https://www.statmt.org/europarl/. It contains translations of parliament proceedings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkcDrW04_JsV"
      },
      "source": [
        "!wget https://object.pouta.csc.fi/OPUS-Europarl/v8/moses/en-ro.txt.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3i-IR3LxATiO"
      },
      "source": [
        "!mkdir data\n",
        "!mv en-ro.txt.zip data/en-ro.txt.zip"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTXeXL-cA4zA"
      },
      "source": [
        "!unzip data/en-ro.txt.zip -d data/\n",
        "!rm data/Europarl.en-ro.xml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WK7Z8DmUBt57"
      },
      "source": [
        "The size of files in lines."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaSl8wlGB5NX"
      },
      "source": [
        "!wc -l data/Europarl*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyBhJ2kBChqY"
      },
      "source": [
        "We shuffle and merge the source and target files horizontally (each line of the resulting file will contain a source line and a target line, separated by a tab). We use the [paste](https://www.geeksforgeeks.org/paste-command-in-linux-with-examples/) command."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFD7dbjdCj1c"
      },
      "source": [
        "!paste data/Europarl.en-ro.ro data/Europarl.en-ro.en | shuf > data/shuf-Europarl.en-ro.both"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIXoEMLLC2gs"
      },
      "source": [
        "with open('data/shuf-Europarl.en-ro.both', 'r', encoding='utf8') as fh:\n",
        "    for i in range(5):\n",
        "        et_sentence, en_sentence = fh.readline().strip().split('\\t')\n",
        "        print('RO: {}\\nEN: {}\\n'.format(et_sentence, en_sentence))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLFsK1na5Vgt"
      },
      "source": [
        "We will use a subset of the Europarl en-ro corpus in our experiments. We separate the data intro train, dev and test using [sed](https://www.geeksforgeeks.org/sed-command-in-linux-unix-with-examples/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9qL0Hqz5F6E"
      },
      "source": [
        "!sed -n 1,20000p data/shuf-Europarl.en-ro.both | cut -f 1 > data/train.ro\n",
        "!sed -n 1,20000p data/shuf-Europarl.en-ro.both | cut -f 2 > data/train.en"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2Ux10Ye6Cr7"
      },
      "source": [
        "!sed -n 20001,21000p data/shuf-Europarl.en-ro.both | cut -f 1 > data/dev.ro\n",
        "!sed -n 20001,21000p data/shuf-Europarl.en-ro.both | cut -f 2 > data/dev.en"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrsFnsdB6Cv4"
      },
      "source": [
        "!sed -n 21001,21500p data/shuf-Europarl.en-ro.both | cut -f 1 > data/test.ro\n",
        "!sed -n 21001,21500p data/shuf-Europarl.en-ro.both | cut -f 2 > data/test.en"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1hOR9FK6XnA"
      },
      "source": [
        "###Text preprocessing\n",
        "\n",
        "We clean the texts and remove bad sentence pairs.\n",
        "\n",
        "* Removing the pairs in which at least one element of the pair (source or target) is empty. \n",
        "* Removing the pairs in which at least one of the sentences has 100 or more words. For simplicity, let's just split the sentences by whitespaces and consider the resulting pieces words; for example, the sentence `How are you?` will consist of 3 \"words\": `[\"How\", \"are\", \"you?\"]`.\n",
        "* Remove the pair if one sentence has at least 5 times as many words as the other one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SI6St6qW6nuR"
      },
      "source": [
        "def clean_sentence_pairs(raw_lines):\n",
        "    # Remove pairs with empty sentences\n",
        "    clean_lines = [pair for pair in raw_lines\n",
        "                  if len(pair[0]) > 0 \n",
        "                  and len(pair[1]) > 0]\n",
        "    print(f'Pairs with empty lines removed: {len(raw_lines) - len(clean_lines)}')\n",
        "    raw_lines = clean_lines\n",
        "\n",
        "    # Remove very long sentences\n",
        "    clean_lines = [pair for pair in raw_lines\n",
        "                  if len(pair[0].split()) < 100 \n",
        "                  and len(pair[1].split()) < 100]\n",
        "    print(f'Pairs with long sentences removed: {len(raw_lines) - len(clean_lines)}')\n",
        "    raw_lines = clean_lines\n",
        "\n",
        "    # Remove pairs with high length ratios\n",
        "    clean_lines = [pair for pair in raw_lines\n",
        "                  if len(pair[0].split())/len(pair[1].split()) < 5\n",
        "                  and len(pair[1].split())/len(pair[0].split()) < 5]\n",
        "    print(f'Pairs with high length ratio removed: {len(raw_lines) - len(clean_lines)}')\n",
        "\n",
        "    return clean_lines"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEfZhUpF7zyg"
      },
      "source": [
        "# Read source and target lines\n",
        "with open('data/train.en', 'r', encoding='utf8') as en_file:\n",
        "    en_lines = [line.strip() for line in en_file]\n",
        "with open('data/train.ro', 'r', encoding='utf8') as et_file:\n",
        "    et_lines = [line.strip() for line in et_file]\n",
        "\n",
        "input_pairs = [(en_lines[i], et_lines[i]) for i in range(len(en_lines))]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYeGcFVn8jBo"
      },
      "source": [
        "# Apply function to sentence pairs\n",
        "cleaned_pairs = clean_sentence_pairs(input_pairs)\n",
        "\n",
        "# Write the result into new files\n",
        "with open('data/cleaned-train.en', 'w', encoding='utf8') as en_clean_file:\n",
        "    en_clean_file.write('\\n'.join([pair[0] for pair in cleaned_pairs]))\n",
        "with open('data/cleaned-train.ro', 'w', encoding='utf8') as et_clean_file:\n",
        "    et_clean_file.write('\\n'.join([pair[1] for pair in cleaned_pairs]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K15ySIn6-NJX"
      },
      "source": [
        "### 4. (Optional) Tokenization\n",
        "\n",
        "In a typical natural language processing pipeline, one of the main pre-processing steps is tokenization. Its task is to turn a string into a list of tokens, in other words, to separate words from punctuation marks (e.g. `Hi, Mary!` $\\rightarrow$ `[\"Hi\", \",\", \"Mary\", \"!\"]`). A typical choice of tokenizer for MT is `mosestokenizer`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obcHnTNuMIcg"
      },
      "source": [
        "!pip install mosestokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26GhFjzh-73R"
      },
      "source": [
        "from mosestokenizer import MosesTokenizer, MosesDetokenizer\n",
        "sentence = \"Am avut posibilitatea de a ne exprima aşteptările.\"\n",
        "\n",
        "with MosesTokenizer('en') as tokenizer:\n",
        "    tok_sentence = tokenizer(sentence)\n",
        "    print(f'Tokenized: {tok_sentence}')\n",
        "with MosesDetokenizer('en') as detokenizer:\n",
        "    detok_sentence = detokenizer(tok_sentence)\n",
        "    print(f'Detokenized: {detok_sentence}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaSou4Yh_RjZ"
      },
      "source": [
        "However, tokenization has some drawbacks. It requires knowledge of patterns of the particular language at hand, and it is not always reversible.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q78mkeX7_T-R"
      },
      "source": [
        "test_sentence = \"Cele mai vechi atestări documentare ale termenului de „rumân/român” cunoscute în mod cert sunt conţinute în relatări, jurnale şi rapoarte de călătorie redactate de umanişti renascentişti din secolul al XVI-lea.\"\n",
        "\n",
        "with MosesTokenizer('en') as tokenizer:\n",
        "    tok_sentence = tokenizer(test_sentence)\n",
        "    print(f'Tokenized: {tok_sentence}')\n",
        "with MosesDetokenizer('en') as detokenizer:\n",
        "    detok_sentence = detokenizer(tok_sentence)\n",
        "    print(f'Detokenized: {detok_sentence}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZnKdYd3_2Qu"
      },
      "source": [
        "As you can see, when we tokenize this sentence and then detokenize it again, it is not reproduced correctly. While this would not be a problem, for example, for text classification, it is a problem for MT. Firstly, we want to have natural-looking output, and secondly, unexpected whitespaces mean that our translations will get a low BLEU score.\n",
        "\n",
        "In this tutorial, we **will not** tokenize our data, because SentencePiece, which we will use for subword segmentation (see section 6 of this notebook) can handle untokenized text. It is also language-independent and reversible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azq-oC3M_3Kr"
      },
      "source": [
        "### 5. (Optional) Truecasing\n",
        "\n",
        "As the next step, we need to deal with capitalization. We have three options:\n",
        "\n",
        "1. Lowercase everything\n",
        "2. Use a truecaser\n",
        "3. Do nothing\n",
        "\n",
        "**Truecasing** is the process of restoring\n",
        "case information to badly-cased or noncased text.\n",
        "\n",
        "We will use the MosesTruecaser from `sacremoses` https://github.com/alvations/sacremoses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypY1KnCQBZ6H"
      },
      "source": [
        "pip install -U sacremoses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rutKjLMRBq-L"
      },
      "source": [
        "We will have to train the truecaser on our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7htrgcR0a5nm"
      },
      "source": [
        "!mkdir model/"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpQtob4KBqXZ"
      },
      "source": [
        "from sacremoses import MosesTruecaser, MosesTokenizer\n",
        "\n",
        "mtr = MosesTruecaser()\n",
        "rotok = MosesTokenizer(lang='ro')\n",
        "\n",
        "tokenized_docs = [rotok.tokenize(line) for line in open('data/cleaned-train.ro')]\n",
        "mtr.train(tokenized_docs, save_to='model/ro.truecasemodel')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yq7DgtMREYr3"
      },
      "source": [
        "mtr = MosesTruecaser()\n",
        "entok = MosesTokenizer(lang='en')\n",
        "\n",
        "tokenized_docs = [entok.tokenize(line) for line in open('data/cleaned-train.en')]\n",
        "mtr.train(tokenized_docs, save_to='model/en.truecasemodel')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Szj6JlCMDZ2k"
      },
      "source": [
        "Using the trained models for truecasing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m-l_Z9_DYnf"
      },
      "source": [
        "!sacremoses -j 14 truecase -m model/ro.truecasemodel < data/cleaned-train.ro > data/tc_cleaned-train.ro\n",
        "!sacremoses -j 14 truecase -m model/en.truecasemodel < data/cleaned-train.en > data/tc_cleaned-train.en"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmL7kIPxE4En"
      },
      "source": [
        "### 6. Subword segmentation\n",
        "\n",
        "The last preprocessing step is subword segmentation. Words will be split into smaller parts based on character co-occurrence frequency. The most common words will remain in one piece, and rare words will be broken into several units.\n",
        "\n",
        "We will use `SentencePiece` https://github.com/google/sentencepiece. Paper: [SentencePiece: A simple and language independent subword tokenizer\n",
        "and detokenizer for Neural Text Processing](https://arxiv.org/pdf/1808.06226.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_RvzYUIMLsE"
      },
      "source": [
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FOppWIqMZf7"
      },
      "source": [
        "We can train a model for splitting our text into subwords. Note that it is common to have a joint vocabulary for source and target languages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFyJ7rHVMd-H"
      },
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "spm.SentencePieceTrainer.train(input=['data/cleaned-train.en', 'data/cleaned-train.ro'],\n",
        "                               model_prefix='model/sentpiece',\n",
        "                               vocab_size=4000)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EM23FRA6MpDy"
      },
      "source": [
        "sp = spm.SentencePieceProcessor(model_file='model/sentpiece.model')\n",
        "encoded = sp.encode('Am avut posibilitatea de a ne exprima aşteptările.', out_type=str)\n",
        "print(encoded)\n",
        "encoded_str = ' '.join(encoded)\n",
        "print(encoded_str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdvNApsKMz-k"
      },
      "source": [
        "We have trained a model with 4000 subwords. This means that SentencePiece will split the words so that the vocabulary size will be 4000."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdV5xs3aM_8W"
      },
      "source": [
        "We will apply the model on the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvOA_XlLM8iz"
      },
      "source": [
        "for filename in ['cleaned-train.en', 'cleaned-train.ro']:\n",
        "    with open(f'data/{filename}', 'r', encoding='utf8') as in_fh:\n",
        "        sp_out = sp.encode([line.strip() for line in in_fh], out_type=str)\n",
        "    with open(f'data/sentpiece-{filename}', 'w', encoding='utf8') as out_fh:\n",
        "        out_fh.writelines([' '.join(line) + '\\n' for line in sp_out])"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCLOxiwbNOZN"
      },
      "source": [
        "with open(f'data/sentpiece-cleaned-train.ro', 'r') as f:\n",
        "    sentpieces_ro = f.readlines()\n",
        "print(sentpieces_ro[:2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfW33jU-NrvG"
      },
      "source": [
        "###Repeat for dev sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZjujyC5N2-_"
      },
      "source": [
        "# Cleaning\n",
        "\n",
        "# Read source and target lines\n",
        "with open('data/dev.en', 'r', encoding='utf8') as en_file:\n",
        "    en_lines = [line.strip() for line in en_file]\n",
        "with open('data/dev.ro', 'r', encoding='utf8') as et_file:\n",
        "    et_lines = [line.strip() for line in et_file]\n",
        "\n",
        "input_pairs = [(en_lines[i], et_lines[i]) for i in range(len(en_lines))]\n",
        "\n",
        "# Apply function to sentence pairs\n",
        "cleaned_pairs = clean_sentence_pairs(input_pairs)\n",
        "\n",
        "# Write the result into new files\n",
        "with open('data/cleaned-dev.en', 'w', encoding='utf8') as en_clean_file:\n",
        "    en_clean_file.write('\\n'.join([pair[0] for pair in cleaned_pairs]))\n",
        "with open('data/cleaned-dev.ro', 'w', encoding='utf8') as et_clean_file:\n",
        "    et_clean_file.write('\\n'.join([pair[1] for pair in cleaned_pairs]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9bJEY-LOBSn"
      },
      "source": [
        "# Subword segmentation\n",
        "for filename in ['cleaned-dev.en', 'cleaned-dev.ro']:\n",
        "    with open(f'data/{filename}', 'r', encoding='utf8') as in_fh:\n",
        "        sp_out = sp.encode([line.strip() for line in in_fh], out_type=str)\n",
        "    with open(f'data/sentpiece-{filename}', 'w', encoding='utf8') as out_fh:\n",
        "        out_fh.writelines([' '.join(line) + '\\n' for line in sp_out])"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCDpBDKqO9wv"
      },
      "source": [
        "We need to write the training and dev data into binary files from which `fairseq` will read during training. And also to build the vocabularies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNt2IAT7PQr6"
      },
      "source": [
        "!fairseq-preprocess --source-lang ro \\\n",
        "                    --target-lang en \\\n",
        "                    --trainpref data/sentpiece-cleaned-train \\\n",
        "                    --validpref data/sentpiece-cleaned-dev \\\n",
        "                    --destdir data/bin-data \\\n",
        "                    --joined-dictionary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfc4LFydPbZP"
      },
      "source": [
        "###Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfIGM0OePiu_"
      },
      "source": [
        "Now that we have preprocessed some texts, we are finally ready to train a translation model. It will not be very good, because we are only using 20,000 sentence pairs for training and we do not have a lot of time, but nevertheless it should learn something useful.\n",
        "\n",
        "Run the command below. It will train a model with 2-layer Transformer encoder and decoder for 10 epochs. This will take some time. (Check that you have selected runtime type 'GPU'.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUogTmwHRKca"
      },
      "source": [
        "!pip uninstall sacrebleu\n",
        "!pip install sacrebleu==1.5.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PozzEeHdmCw"
      },
      "source": [
        "!pip install tensorboardX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QaddsZ_PsFa"
      },
      "source": [
        "!fairseq-train data/bin-data --arch transformer \\\n",
        "                             --lr 0.005 \\\n",
        "                             --encoder-attention-heads 4 \\\n",
        "                             --encoder-embed-dim 32 \\\n",
        "                             --encoder-layers 2 \\\n",
        "                             --encoder-ffn-embed-dim 64 \\\n",
        "                             --decoder-attention-heads 4 \\\n",
        "                             --decoder-embed-dim 32 \\\n",
        "                             --decoder-layers 2 \\\n",
        "                             --decoder-ffn-embed-dim 64 \\\n",
        "                             --max-epoch 10 \\\n",
        "                             --optimizer adam \\\n",
        "                             --max-tokens 4000 \\\n",
        "                             --save-dir data/et2en_model \\\n",
        "                             --log-format json \\\n",
        "                             --tensorboard-logdir data/et2en_model/log-tb \\\n",
        "                             --eval-bleu \\\n",
        "                             --eval-bleu-remove-bpe=sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb_QB2w2RvzX"
      },
      "source": [
        "Given the small training set, we got low BLEU scores at validation, which indicates that our model did not learn to translate well yet, but is not generating completely random output either."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "544CvVreRu1q"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir data/et2en_model/log-tb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uj9Qa2p7R9qo"
      },
      "source": [
        "###Translating"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmlfKPJ9SGT8"
      },
      "source": [
        "Before we can translate a sentence, we need to preprocess it in the same way as we did the training and development sets.\n",
        "\n",
        "We will create the file with our sample text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8fMsFVBSFIg"
      },
      "source": [
        "!echo \"România este un stat situat în sud-estul Europei Centrale, pe cursul inferior al Dunării\" >> data/input.ro\n",
        "!echo \"România a apărut ca stat, condus de Alexandru Ioan Cuza, în 1859.\" >> data/input.ro\n",
        "!echo \"A fost recunoscută ca ţară independentă 19 ani mai târziu.\" >> data/input.ro"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOpD1TMZS9Ex"
      },
      "source": [
        "Preprocessing the input text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKy1DyKeTE-H"
      },
      "source": [
        "# Subword segmentation\n",
        "with open(f'data/input.ro', 'r', encoding='utf8') as in_fh:\n",
        "    sp_out = sp.encode([line.strip() for line in in_fh], out_type=str)\n",
        "with open(f'data/sentpiece-input.ro', 'w', encoding='utf8') as out_fh:\n",
        "    out_fh.writelines([' '.join(line) + '\\n' for line in sp_out])"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7qwr6t7TN7l"
      },
      "source": [
        "Now we can translate it. To get a readable sentence, we also need to reverse SentencePiece splitting afterwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uukbfGoFTOru"
      },
      "source": [
        "!cat data/sentpiece-input.ro | fairseq-interactive data/bin-data \\\n",
        "                                               --source-lang ro \\\n",
        "                                               --target-lang en \\\n",
        "                                               --path data/et2en_model/checkpoint_best.pt \\\n",
        "                                               > data/output.en"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVz9flaNThl6"
      },
      "source": [
        "!cat data/output.en"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXgLsvXuTzAo"
      },
      "source": [
        "!grep \"^H\" data/output.en | cut -f3 > data/hypothesis.en"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGRGCBNHT2Wv"
      },
      "source": [
        "# De-SentencePiece\n",
        "with open(f'data/hypothesis.en', 'r', encoding='utf8') as in_fh:\n",
        "    de_sp_out = [sp.decode(line.strip().split()) for line in in_fh]\n",
        "    print(de_sp_out)\n",
        "with open(f'data/de-sentpiece-hypothesis.en', 'w', encoding='utf8') as out_fh:\n",
        "    out_fh.writelines([line + '\\n' for line in de_sp_out])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4UHuXmHUTX9"
      },
      "source": [
        "!cat data/de-sentpiece-hypothesis.en"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2i4X8gLyUpI8"
      },
      "source": [
        "You can probably see that our model generates readable English text, but it is not necessarily a translation of the input. The language model component is already OK, but the conditioning part is not working yet. You will fix it when you train a bigger baseline with more data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCKEFWZNUqEl"
      },
      "source": [
        "##Assignment\n",
        "\n",
        "To be uploaded here: https://forms.gle/A8rHbckQcfZfRMJMA\n",
        "\n",
        "Try to experiment with other framework(s) for machine translation. You can use the list of resources from the begging of the lab or any other framework you know for NMT.\n",
        "\n",
        "You can use [Europarl data](https://www.statmt.org/europarl/) or experiment with other data from  https://www.statmt.org/.\n",
        "\n",
        "Split the data you are using into train/dev/test and report your performance in terms on BLEU score and/or other MT metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8i-7NVX5voqm"
      },
      "source": [
        "###Resources\n",
        "\n",
        "* [Intro to Pytorch](https://github.com/udacity/deep-learning-v2-pytorch/tree/master/intro-to-pytorch)\n",
        "* [Pytorch Lightning](https://github.com/PyTorchLightning/pytorch-lightning)\n",
        "* [Intro to Tensorflow](https://github.com/udacity/intro-to-ml-tensorflow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kU4Ag2uJ-GDr"
      },
      "source": [
        "Notebook adapted from: [MTAT.06.055 Machine Translation](https://courses.cs.ut.ee/2021/mt/spring/Main/HomePage)"
      ]
    }
  ]
}