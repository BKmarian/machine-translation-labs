{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab2_MT.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPeJCNADP2BAuCnlch21E/K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bucuram/machine-translation-labs/blob/main/Lab2_MT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjiNn4XDtESN"
      },
      "source": [
        "##Overview of Approaches to MT\n",
        "\n",
        "### Open NMT frameworks\n",
        "* [Moses](http://www.statmt.org/moses/https://aclanthology.org/P07-2045.pdf). Paper: [Moses: Open Source Toolkit for Statistical Machine Translation](https://aclanthology.org/P07-2045.pdf). C++\n",
        "\n",
        "* [OpenNMT](https://github.com/OpenNMT/OpenNMT-py). Paper: [OpenNMT: Open-Source Toolkit for Neural Machine Translation](https://aclanthology.org/P17-4012.pdf). PyTorch / TensorFlow. Developed by Harvard NLP,  SYSTRAN\n",
        "* [Marian](https://marian-nmt.github.io/). Paper: [Marian: Fast Neural Machine Translation in C++](https://aclanthology.org/P18-4020.pdf). C++. Developed by Microsoft Translator\n",
        "* [Fairseq](https://github.com/pytorch/fairseq). Paper: [https://aclanthology.org/N19-4009.pdf](https://aclanthology.org/N19-4009.pdf). PyTorch. Developed by Facebook AI\n",
        "* [Nematus](https://github.com/EdinburghNLP/nematus). Paper: [Nematus: a Toolkit for Neural Machine Translation](https://aclanthology.org/E17-3017.pdf). TensorFlow. Developed by Edinburgh NLP\n",
        "* [Sockeye](https://github.com/awslabs/sockeye). Paper: [SOCKEYE 2:A Toolkit for Neural Machine Translation](https://aclanthology.org/2020.eamt-1.50.pdf). MXNet. Developed by Amazon\n",
        "* [JoeyNMT](https://github.com/joeynmt/joeynmt). Paper: [Joey NMT: A Minimalist NMT Toolkit for Novices](https://aclanthology.org/D19-3019v1.pdf). PyTorch\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efi_dqwm-8fb"
      },
      "source": [
        "###Testing the fairseq framework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSTgKz3cBlXa"
      },
      "source": [
        "Installing fairseq, mosestokenizer and tensorboardX"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "g44okfSZ-03H",
        "outputId": "5045a241-ad36-43a0-af40-fde19432ade7"
      },
      "source": [
        "!pip install sentencepiece fairseq tensorboardX mosestokenizer"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 5.1 MB/s \n",
            "\u001b[?25hCollecting fairseq\n",
            "  Downloading fairseq-0.10.2-cp37-cp37m-manylinux1_x86_64.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 29.0 MB/s \n",
            "\u001b[?25hCollecting tensorboardX\n",
            "  Downloading tensorboardX-2.4-py2.py3-none-any.whl (124 kB)\n",
            "\u001b[K     |████████████████████████████████| 124 kB 57.6 MB/s \n",
            "\u001b[?25hCollecting mosestokenizer\n",
            "  Downloading mosestokenizer-1.1.0.tar.gz (37 kB)\n",
            "Collecting dataclasses\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from fairseq) (1.9.0+cu111)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from fairseq) (4.62.3)\n",
            "Collecting sacrebleu>=1.4.12\n",
            "  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\n",
            "\u001b[K     |████████████████████████████████| 90 kB 8.7 MB/s \n",
            "\u001b[?25hCollecting hydra-core\n",
            "  Downloading hydra_core-1.1.1-py3-none-any.whl (145 kB)\n",
            "\u001b[K     |████████████████████████████████| 145 kB 56.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fairseq) (1.19.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from fairseq) (2019.12.20)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.7/dist-packages (from fairseq) (1.14.6)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from fairseq) (0.29.24)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.3.2-py2.py3-none-any.whl (15 kB)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.8.9)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from mosestokenizer) (0.6.2)\n",
            "Collecting openfile\n",
            "  Downloading openfile-0.0.7-py3-none-any.whl (2.4 kB)\n",
            "Collecting uctools\n",
            "  Downloading uctools-1.3.0.tar.gz (4.6 kB)\n",
            "Collecting toolwrapper\n",
            "  Downloading toolwrapper-2.1.0.tar.gz (3.2 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi->fairseq) (2.20)\n",
            "Collecting omegaconf==2.1.*\n",
            "  Downloading omegaconf-2.1.1-py3-none-any.whl (74 kB)\n",
            "\u001b[K     |████████████████████████████████| 74 kB 3.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from hydra-core->fairseq) (5.2.2)\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 53.2 MB/s \n",
            "\u001b[?25hCollecting PyYAML>=5.1.0\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 51.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->hydra-core->fairseq) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->fairseq) (3.7.4.3)\n",
            "Building wheels for collected packages: mosestokenizer, antlr4-python3-runtime, toolwrapper, uctools\n",
            "  Building wheel for mosestokenizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mosestokenizer: filename=mosestokenizer-1.1.0-py3-none-any.whl size=49117 sha256=31f331cddb37e687717ae66385f681c04dc3167cee646992ce05f0ed50574cfa\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/31/94/fef279382208e85a65c1a7f5c4d0020115477b0af74f296b57\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141230 sha256=5b65784dbd43f5c30fac5461606dbd355c28d0ff153fdc38091d28171ee3073b\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/33/b7/336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n",
            "  Building wheel for toolwrapper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for toolwrapper: filename=toolwrapper-2.1.0-py3-none-any.whl size=3354 sha256=a179064d44eafe66f82ceeb22ff14755ad069900071f552004465abe42973826\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/4f/33/54741ffe08e38ececb1d28068a153729b4fe820bafa0a0691f\n",
            "  Building wheel for uctools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for uctools: filename=uctools-1.3.0-py3-none-any.whl size=6163 sha256=3519f6ec83b509fa8165e9bf4e652de38006936d31785462eee523a5e410b432\n",
            "  Stored in directory: /root/.cache/pip/wheels/fb/44/e9/914cf8fa71f0141f9314f862538d1218fcf2b94542a0fb7d35\n",
            "Successfully built mosestokenizer antlr4-python3-runtime toolwrapper uctools\n",
            "Installing collected packages: PyYAML, antlr4-python3-runtime, portalocker, omegaconf, colorama, uctools, toolwrapper, sacrebleu, openfile, hydra-core, dataclasses, tensorboardX, sentencepiece, mosestokenizer, fairseq\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-6.0 antlr4-python3-runtime-4.8 colorama-0.4.4 dataclasses-0.6 fairseq-0.10.2 hydra-core-1.1.1 mosestokenizer-1.1.0 omegaconf-2.1.1 openfile-0.0.7 portalocker-2.3.2 sacrebleu-2.0.0 sentencepiece-0.1.96 tensorboardX-2.4 toolwrapper-2.1.0 uctools-1.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKzw-dpE_Kaz"
      },
      "source": [
        "Downloading the data\n",
        "\n",
        "We will use the Europarl parallel corpus https://www.statmt.org/europarl/. It contains translations of parliament proceedings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkcDrW04_JsV",
        "outputId": "bbc0e9e3-7e2a-42ee-c3ed-01cb629c5292"
      },
      "source": [
        "!wget https://object.pouta.csc.fi/OPUS-Europarl/v8/moses/en-ro.txt.zip"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-20 14:04:41--  https://object.pouta.csc.fi/OPUS-Europarl/v8/moses/en-ro.txt.zip\n",
            "Resolving object.pouta.csc.fi (object.pouta.csc.fi)... 86.50.254.18, 86.50.254.19\n",
            "Connecting to object.pouta.csc.fi (object.pouta.csc.fi)|86.50.254.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 39495951 (38M) [application/zip]\n",
            "Saving to: ‘en-ro.txt.zip’\n",
            "\n",
            "en-ro.txt.zip       100%[===================>]  37.67M  15.4MB/s    in 2.4s    \n",
            "\n",
            "2021-10-20 14:04:44 (15.4 MB/s) - ‘en-ro.txt.zip’ saved [39495951/39495951]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3i-IR3LxATiO"
      },
      "source": [
        "!mkdir data\n",
        "!mv en-ro.txt.zip data/en-ro.txt.zip"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTXeXL-cA4zA"
      },
      "source": [
        "%cd data/\n",
        "!unzip en-ro.txt.zip\n",
        "!rm Europarl.en-ro.xml"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WK7Z8DmUBt57"
      },
      "source": [
        "Let's check how many lines our files contain:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JaSl8wlGB5NX",
        "outputId": "a9e9bcfe-92d7-4eb4-e5ee-6d98e1ed9dd1"
      },
      "source": [
        "!wc -l Europarl*"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   400356 Europarl.en-ro.en\n",
            "   400356 Europarl.en-ro.ro\n",
            "   800712 total\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyBhJ2kBChqY"
      },
      "source": [
        "Let's see what some random sentence pairs from this corpus look like. First, let's shuffle and merge the source and target files horizontally (each line of the resulting file will contain a source line and a target line, separated by a tab):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFD7dbjdCj1c"
      },
      "source": [
        "!paste Europarl.en-ro.ro Europarl.en-ro.en | shuf > shuf-Europarl.en-ro.both"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIXoEMLLC2gs",
        "outputId": "2fa0288a-8679-4e38-82bd-ce3a93d39b7f"
      },
      "source": [
        "with open('shuf-Europarl.en-ro.both', 'r', encoding='utf8') as fh:\n",
        "    for i in range(5):\n",
        "        et_sentence, en_sentence = fh.readline().strip().split('\\t')\n",
        "        print('RO: {}\\nEN: {}\\n'.format(et_sentence, en_sentence))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RO: Ca toţi ceilalţi din această cameră, sunt, desigur, oripilat de ceea ce se întâmplă în Orientul Mijlociu.\n",
            "EN: Like everybody else in this Chamber, I am, of course, horrified by what has been happening in the Middle East.\n",
            "\n",
            "RO: În ceea ce mă privește, voi trata astfel de situații respectând termenele limită care îmi sunt impuse.\n",
            "EN: I will abide by my deadlines in terms of dealing with this kind of situation.\n",
            "\n",
            "RO: Scopul este acela de a îmbunătăți capacitatea Uniunii Europene de a acționa într-un rol de gestionare a situațiilor de criză, permițând furnizarea și utilizarea mai eficientă a resurselor financiare, civile și militare.\n",
            "EN: The aim is to improve the ability of the European Union to act in a crisis management role by allowing financial, civilian and military resources to be provided and used more efficiently.\n",
            "\n",
            "RO: Avem nevoie de o revoluţie verde şi trebuie să ne frânăm excesele din prezent.\n",
            "EN: We need a green revolution and we must curb our own excesses.\n",
            "\n",
            "RO: membru al Comisiei. - Dle președinte, dezbaterea de astăzi privind Islanda și următoarele etape ale procesului de aderare este foarte oportună.\n",
            "EN: Member of the Commission. - Mr President, today's debate on Iceland and the next steps in its accession process is very opportune.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8i-7NVX5voqm"
      },
      "source": [
        "###Resources\n",
        "\n",
        "* [Intro to Pytorch](https://github.com/udacity/deep-learning-v2-pytorch/tree/master/intro-to-pytorch)\n",
        "* [Pytorch Lightning](https://github.com/PyTorchLightning/pytorch-lightning)\n",
        "* [Intro to Tensorflow](https://github.com/udacity/intro-to-ml-tensorflow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kU4Ag2uJ-GDr"
      },
      "source": [
        "Notebook adapted from: [MTAT.06.055 Machine Translation](https://courses.cs.ut.ee/2021/mt/spring/Main/HomePage)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ow3BlB74vqyn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}